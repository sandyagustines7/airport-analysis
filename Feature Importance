from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.feature_selection import VarianceThreshold
from sklearn.preprocessing import OrdinalEncoder
import pandas as pd

# Load processed dataset
df = pd.read_csv('new_features_data.csv')

# Define features (X) and target (y), exclude 'CANCELLATION_CODE' and 'CANCELLATION_REASON_CODE'
X = df.drop(columns=['DEP_DELAY', 'CANCELLATION_CODE', 'CANCELLATION_REASON_CODE'])
y = df['DEP_DELAY']

# Ordinal encoding for delay categories
delay_mapping = {
    'Early Departure': 0,
    '0-15 min': 1,
    '15-30 min': 2,
    '30+ min': 3
}
if 'DEP_DELAY_CATEGORY' in X.columns and 'ARR_DELAY_CATEGORY' in X.columns:
    X['DEP_DELAY_CATEGORY'] = X['DEP_DELAY_CATEGORY'].map(delay_mapping)
    X['ARR_DELAY_CATEGORY'] = X['ARR_DELAY_CATEGORY'].map(delay_mapping)

# Convert datetime columns to numeric features
datetime_cols = ['DEP_TIME', 'ARR_TIME', 'CRS_DEP_TIME', 'CRS_ARR_TIME']
for col in datetime_cols:
    if col in X.columns:
        # Extract hour and minute components
        X[f'{col}_HOUR'] = pd.to_datetime(X[col]).dt.hour
        X[f'{col}_MINUTE'] = pd.to_datetime(X[col]).dt.minute

# Drop the original datetime columns
X.drop(columns=datetime_cols, inplace=True, errors='ignore')

# Handle any remaining missing values
X.fillna('null', inplace=True)  # Replace remaining missing values with 'null'

# Check dataset dimensions and memory usage
print(f"Feature matrix shape: {X.shape}")
print(f"Target vector shape: {y.shape}")
print(f"Feature matrix memory usage: {X.memory_usage(deep=True).sum() / 1e6:.2f} MB")
print(f"Target vector memory usage: {y.memory_usage(deep=True) / 1e6:.2f} MB")

# Remove low-variance features
numeric_cols = X.select_dtypes(include=['number']).columns
X_numeric = X[numeric_cols]
selector = VarianceThreshold(threshold=0.01)
X_numeric_reduced = selector.fit_transform(X_numeric)
X_reduced = pd.DataFrame(X_numeric_reduced, columns=numeric_cols[selector.get_support()])
print(f"Reduced feature matrix shape: {X_reduced.shape}")

# Use ordinal encoding for any remaining categorical variables
categorical_cols = X.select_dtypes(include=['object']).columns

# Replace missing values in categorical columns with 'null'
X[categorical_cols] = X[categorical_cols].fillna('null')

# Ensure all categorical columns are strings
for col in categorical_cols:
    X[col] = X[col].astype(str)

# Apply OrdinalEncoder
if len(categorical_cols) > 0:
    encoder = OrdinalEncoder()
    X[categorical_cols] = encoder.fit_transform(X[categorical_cols])

# Combine numeric and encoded categorical features
X_final = pd.concat([X_reduced, X[categorical_cols]], axis=1)

# Optional: Sample data to reduce size
X_sampled = X_final.sample(frac=0.5, random_state=42)  # Adjust fraction if needed
y_sampled = y.loc[X_sampled.index]

# Split data into training and testing
X_train, X_test, y_train, y_test = train_test_split(X_sampled, y_sampled, test_size=0.2, random_state=42)

# Train a Random Forest Regressor with optimized parameters
rf = RandomForestRegressor(random_state=42, n_estimators=50, max_depth=10, max_features='sqrt')
rf.fit(X_train, y_train)

# Evaluate feature importance
importance = rf.feature_importances_
importance_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': importance})
importance_df = importance_df.sort_values(by='Importance', ascending=False)

# Display top features
print("Top Features:")
print(importance_df.head(10))

# Save the processed features and target variable for later use
X_final.to_csv('processed_features.csv', index=False)
y.to_csv('processed_target.csv', index=False)
importance_df.to_csv('feature_importance.csv', index=False)
